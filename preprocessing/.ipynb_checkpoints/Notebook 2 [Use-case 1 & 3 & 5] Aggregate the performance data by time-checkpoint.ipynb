{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a379dbbf",
   "metadata": {},
   "source": [
    "# 5G-IOT visual interface - Data Cleaning and Transformation\n",
    "\n",
    "This notebook is developed to process application-dependent datasets, collected by the trucks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321f3fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import geopy\n",
    "import time\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'figure.figsize':(7,5), 'figure.dpi':100})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d5946f",
   "metadata": {},
   "source": [
    "### Define Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fc6bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data folder source\n",
    "base = \"../data/sources/app-dependent/\"\n",
    "\n",
    "# Column rename map\n",
    "rename_map = {\n",
    "    \"Bitrate\": \"Upload speed (Mbits/sec)\",\n",
    "    \"Bitrate-RX\": \"Download speed (MBits/sec)\",\n",
    "    \"Transfer size\": \"Upload size (MBytes)\",\n",
    "    \"Transfer size-RX\": \"Download size (MBytes)\",\n",
    "    \"send_data\": \"Upload throughput (MBits)\",\n",
    "    \"svr1\": \"Response time from CloudflareDNS (ms)\",\n",
    "    \"svr2\": \"Response time from GoogleDNS (ms)\",\n",
    "    \"svr3\": \"Response time from OptusDNS (ms)\",\n",
    "    \"svr4\": \"Response time from EC2DNS (ms)\"\n",
    "}\n",
    "\n",
    "# Truck number\n",
    "trucks = list(range(1,12))\n",
    "\n",
    "# Truck operation dates\n",
    "dates = [\n",
    "    \"2022-07-06\",\n",
    "    \"2022-07-07\",\n",
    "    \"2022-07-08\",\n",
    "    \"2022-07-11\",\n",
    "    \"2022-07-12\",\n",
    "    \"2022-07-13\",\n",
    "    \"2022-07-14\",\n",
    "    \"2022-07-15\",\n",
    "    \"2022-07-18\",\n",
    "    \"2022-07-19\",\n",
    "    \"2022-07-20\",\n",
    "    \"2022-07-21\",\n",
    "    \"2022-07-22\",\n",
    "]\n",
    "\n",
    "# performance columns\n",
    "performance_columns = {\n",
    "    'Bitrate': ['mean'],\n",
    "    'Bitrate-RX': ['mean'],\n",
    "    'Transfer size-RX': ['mean'],\n",
    "    'Transfer size': ['mean'],\n",
    "    'send_data': ['mean'],\n",
    "    'speed': ['mean'],\n",
    "    'svr1': ['mean'],\n",
    "    'svr2': ['mean'],\n",
    "    'svr3': ['mean'],\n",
    "    'svr4': ['mean']\n",
    "}\n",
    "\n",
    "#\n",
    "geo_columns = {\n",
    "    'latitude': ['mean'],\n",
    "    'longitude': ['mean']\n",
    "}\n",
    "\n",
    "performance_location_columns = {\n",
    "    **performance_columns,\n",
    "    **geo_columns\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67427780",
   "metadata": {},
   "source": [
    "### Examine a data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7ef9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['time', 'Day', 'Year', 'Month', 'Date', 'hour', 'min', 'sec',\n",
      "       'timezone', 'latitude', 'longitude', 'speed', 'truck', 'svr1', 'svr2',\n",
      "       'svr3', 'svr4', 'Role', 'Transfer size', 'Transfer unit', 'Bitrate',\n",
      "       'bitrate_unit', 'Retransmissions', 'CWnd', 'cwnd_unit', 'Role-RX',\n",
      "       'Transfer size-RX', 'Transfer unit-RX', 'Bitrate-RX', 'bitrate_unit-RX',\n",
      "       'send_data'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>Day</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Date</th>\n",
       "      <th>hour</th>\n",
       "      <th>min</th>\n",
       "      <th>sec</th>\n",
       "      <th>timezone</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>bitrate_unit</th>\n",
       "      <th>Retransmissions</th>\n",
       "      <th>CWnd</th>\n",
       "      <th>cwnd_unit</th>\n",
       "      <th>Role-RX</th>\n",
       "      <th>Transfer size-RX</th>\n",
       "      <th>Transfer unit-RX</th>\n",
       "      <th>Bitrate-RX</th>\n",
       "      <th>bitrate_unit-RX</th>\n",
       "      <th>send_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1658455306</td>\n",
       "      <td>Fri</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>AEST</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1658455306</td>\n",
       "      <td>Fri</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>AEST</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1658455306</td>\n",
       "      <td>Fri</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>AEST</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1658455306</td>\n",
       "      <td>Fri</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>AEST</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1658455306</td>\n",
       "      <td>Fri</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>AEST</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>1658459117</td>\n",
       "      <td>Fri</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>AEST</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>1658459118</td>\n",
       "      <td>Fri</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>AEST</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>1658459119</td>\n",
       "      <td>Fri</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>AEST</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>1658459120</td>\n",
       "      <td>Fri</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>AEST</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>1658459121</td>\n",
       "      <td>Fri</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>AEST</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           time  Day    Year  Month  Date  hour  min   sec timezone  latitude  \\\n",
       "0    1658455306  Fri  2022.0    7.0  22.0  12.0  1.0  46.0     AEST      99.0   \n",
       "1    1658455306  Fri  2022.0    7.0  22.0  12.0  1.0  46.0     AEST      99.0   \n",
       "2    1658455306  Fri  2022.0    7.0  22.0  12.0  1.0  46.0     AEST      99.0   \n",
       "3    1658455306  Fri  2022.0    7.0  22.0  12.0  1.0  46.0     AEST      99.0   \n",
       "4    1658455306  Fri  2022.0    7.0  22.0  12.0  1.0  46.0     AEST      99.0   \n",
       "..          ...  ...     ...    ...   ...   ...  ...   ...      ...       ...   \n",
       "576  1658459117  Fri  2022.0    7.0  22.0  13.0  5.0  17.0     AEST      99.0   \n",
       "577  1658459118  Fri  2022.0    7.0  22.0  13.0  5.0  18.0     AEST      99.0   \n",
       "578  1658459119  Fri  2022.0    7.0  22.0  13.0  5.0  19.0     AEST      99.0   \n",
       "579  1658459120  Fri  2022.0    7.0  22.0  13.0  5.0  20.0     AEST      99.0   \n",
       "580  1658459121  Fri  2022.0    7.0  22.0  13.0  5.0  21.0     AEST      99.0   \n",
       "\n",
       "     ...  bitrate_unit  Retransmissions CWnd  cwnd_unit  Role-RX  \\\n",
       "0    ...           NaN              NaN  NaN        NaN      NaN   \n",
       "1    ...           NaN              NaN  NaN        NaN      NaN   \n",
       "2    ...           NaN              NaN  NaN        NaN      NaN   \n",
       "3    ...           NaN              NaN  NaN        NaN      NaN   \n",
       "4    ...           NaN              NaN  NaN        NaN      NaN   \n",
       "..   ...           ...              ...  ...        ...      ...   \n",
       "576  ...           NaN              NaN  NaN        NaN      NaN   \n",
       "577  ...           NaN              NaN  NaN        NaN      NaN   \n",
       "578  ...           NaN              NaN  NaN        NaN      NaN   \n",
       "579  ...           NaN              NaN  NaN        NaN      NaN   \n",
       "580  ...           NaN              NaN  NaN        NaN      NaN   \n",
       "\n",
       "     Transfer size-RX  Transfer unit-RX Bitrate-RX  bitrate_unit-RX send_data  \n",
       "0                 NaN               NaN        NaN              NaN       NaN  \n",
       "1                 NaN               NaN        NaN              NaN       NaN  \n",
       "2                 NaN               NaN        NaN              NaN       NaN  \n",
       "3                 NaN               NaN        NaN              NaN       NaN  \n",
       "4                 NaN               NaN        NaN              NaN       NaN  \n",
       "..                ...               ...        ...              ...       ...  \n",
       "576               NaN               NaN        NaN              NaN  0.002518  \n",
       "577               NaN               NaN        NaN              NaN  0.004253  \n",
       "578               NaN               NaN        NaN              NaN  0.002266  \n",
       "579               NaN               NaN        NaN              NaN  0.000189  \n",
       "580               NaN               NaN        NaN              NaN       NaN  \n",
       "\n",
       "[581 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(base + \"/2022-07-22/2022-07-22-garbo03-combined.log\")\n",
    "print(\"Columns: \", raw_data.columns)\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ab9561",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff579e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_CWnd(row):\n",
    "    try:\n",
    "        return float(row.CWnd)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def adjust_send_data(row):\n",
    "    try:\n",
    "        # multiply with 8 to get MBits value\n",
    "        return row.get('send_data', 0) * 8\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def clean(data, filter_location = False):\n",
    "    temp = data.copy()\n",
    "    if 'time' in temp.columns:\n",
    "        temp = temp.drop(columns=['time'])\n",
    "    \n",
    "    if 'Time' in temp.columns:\n",
    "        temp = temp.drop(columns=['Time'])\n",
    "\n",
    "    non_null_data = temp.dropna(subset=['Day', 'Year', 'Month', 'hour', 'min', 'sec']).copy()\n",
    "    non_null_data['send_data'] = non_null_data.apply(lambda r: adjust_send_data(r), axis=1)\n",
    "    non_null_data['CWnd'] = non_null_data.apply(lambda r: adjust_CWnd(r), axis=1)\n",
    "    if filter_location:\n",
    "        # remove the invalid location\n",
    "        return non_null_data.query(\"latitude < -37 & latitude > -38 & longitude > 144 & longitude < 145\")\n",
    "    else:\n",
    "        return non_null_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865cf4c0",
   "metadata": {},
   "source": [
    "### Aggregation by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70595dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_date(data): \n",
    "    aggregated_data = data.copy()\n",
    "    groupby_columns = ['Year', 'Month', 'Date', 'truck']\n",
    "    aggregated_data = aggregated_data.groupby(groupby_columns, as_index=False).agg(performance_columns)\n",
    "    aggregated_data.columns = aggregated_data.columns.get_level_values(0)\n",
    "    return aggregated_data\n",
    "\n",
    "def aggregate_by_hour(data): \n",
    "    aggregated_data = data.copy()\n",
    "    \n",
    "    # group columns to perform aggregation\n",
    "    groupby_columns = ['Year', 'Month', 'Date', 'truck', 'hour']\n",
    "    aggregated_data = aggregated_data.groupby(groupby_columns, as_index=False).agg(performance_columns)\n",
    "    \n",
    "    # flatten the columns to view the data properly\n",
    "    aggregated_data.columns = aggregated_data.columns.get_level_values(0)\n",
    "    return aggregated_data\n",
    "\n",
    "def aggregate_by_minute(data): \n",
    "    aggregated_data = data.copy()\n",
    "    groupby_columns = ['truck', 'hour', 'Date', 'Month', 'Year', 'min' ]\n",
    "    aggregated_data = aggregated_data.groupby(groupby_columns, as_index=False).agg(performance_location_columns)\n",
    "    aggregated_data.columns = aggregated_data.columns.get_level_values(0)\n",
    "    return aggregated_data\n",
    "\n",
    "def aggregate_by_second(data):\n",
    "    aggregated_data = data.copy()\n",
    "    aggregated_data = aggregated_data.groupby(groupby_columns, as_index=False).agg(performance_location_columns)\n",
    "    aggregated_data.columns = aggregated_data.columns.get_level_values(0)\n",
    "    return aggregated_data\n",
    "\n",
    "def pipeline_by_minute(data): \n",
    "    cleaned = clean(data, filter_location=True)\n",
    "    return aggregate_by_minute(cleaned)\n",
    "\n",
    "def pipeline_by_hour(data): \n",
    "    cleaned = clean(data)\n",
    "    return aggregate_by_hour(cleaned)\n",
    "\n",
    "def pipeline_by_date(data):\n",
    "    cleaned = clean(data)\n",
    "    return aggregate_by_date(cleaned)\n",
    "\n",
    "def pipeline_by_second(data):\n",
    "    cleaned = clean(data, filter_location=True)\n",
    "    return aggregate_by_second(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc46bf",
   "metadata": {},
   "source": [
    "## Other util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2220f52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/sources/app-dependent//2022-07-06/2022-07-06-garbo01-combined.log'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dataset_name(date, truck, base_uri):\n",
    "    truck_name = 'garbo' + (('0' + str(truck)) if truck < 10 else str(truck)) \n",
    "    return base_uri + \"/\" + date + \"/\" + date + \"-\" + truck_name + '-combined.log'\n",
    "\n",
    "get_dataset_name(\"2022-07-06\", 1, base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f974ae",
   "metadata": {},
   "source": [
    "## Execute the aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0018cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"../data/outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bedc2fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hodac\\AppData\\Local\\Temp\\ipykernel_11164\\4212648226.py:35: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for date in tqdm(dates):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153f1fa3050e4d27b44c220ee6865d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hodac\\AppData\\Local\\Temp\\ipykernel_11164\\4212648226.py:39: DtypeWarning: Columns (1,8,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  datasets.append(pipeline_by_minute(pd.read_csv(dataset_name)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other error:  ../data/sources/app-dependent//2022-07-08/2022-07-08-garbo01-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-08/2022-07-08-garbo01-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-08/2022-07-08-garbo07-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-08/2022-07-08-garbo07-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-08/2022-07-08-garbo08-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-08/2022-07-08-garbo08-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-08/2022-07-08-garbo11-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-08/2022-07-08-garbo11-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-11/2022-07-11-garbo02-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-11/2022-07-11-garbo02-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-12/2022-07-12-garbo02-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-12/2022-07-12-garbo02-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-12/2022-07-12-garbo07-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-12/2022-07-12-garbo07-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-12/2022-07-12-garbo08-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-12/2022-07-12-garbo08-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-13/2022-07-13-garbo02-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-13/2022-07-13-garbo02-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-13/2022-07-13-garbo03-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-13/2022-07-13-garbo03-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-13/2022-07-13-garbo07-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-13/2022-07-13-garbo07-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-14/2022-07-14-garbo02-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-14/2022-07-14-garbo02-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-14/2022-07-14-garbo07-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-14/2022-07-14-garbo07-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-15/2022-07-15-garbo02-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-15/2022-07-15-garbo02-combined.log'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hodac\\AppData\\Local\\Temp\\ipykernel_11164\\4212648226.py:39: DtypeWarning: Columns (1,8,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  datasets.append(pipeline_by_minute(pd.read_csv(dataset_name)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other error:  ../data/sources/app-dependent//2022-07-15/2022-07-15-garbo07-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-15/2022-07-15-garbo07-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-15/2022-07-15-garbo10-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-15/2022-07-15-garbo10-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-18/2022-07-18-garbo02-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-18/2022-07-18-garbo02-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-18/2022-07-18-garbo07-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-18/2022-07-18-garbo07-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-18/2022-07-18-garbo10-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-18/2022-07-18-garbo10-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-19/2022-07-19-garbo02-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-19/2022-07-19-garbo02-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-19/2022-07-19-garbo03-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-19/2022-07-19-garbo03-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-19/2022-07-19-garbo10-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-19/2022-07-19-garbo10-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-20/2022-07-20-garbo02-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-20/2022-07-20-garbo02-combined.log'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hodac\\AppData\\Local\\Temp\\ipykernel_11164\\4212648226.py:39: DtypeWarning: Columns (1,8,12,17,19,21,24,25,27,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  datasets.append(pipeline_by_minute(pd.read_csv(dataset_name)))\n",
      "C:\\Users\\hodac\\AppData\\Local\\Temp\\ipykernel_11164\\4212648226.py:39: DtypeWarning: Columns (1,8,12,17,19,21,24,25,27,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  datasets.append(pipeline_by_minute(pd.read_csv(dataset_name)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other error:  ../data/sources/app-dependent//2022-07-20/2022-07-20-garbo10-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-20/2022-07-20-garbo10-combined.log'\n",
      "Other error:  ../data/sources/app-dependent//2022-07-21/2022-07-21-garbo08-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-21/2022-07-21-garbo08-combined.log'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hodac\\AppData\\Local\\Temp\\ipykernel_11164\\4212648226.py:39: DtypeWarning: Columns (1,8,12,17,19,21,24,25,27,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  datasets.append(pipeline_by_minute(pd.read_csv(dataset_name)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other error:  ../data/sources/app-dependent//2022-07-21/2022-07-21-garbo10-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-21/2022-07-21-garbo10-combined.log'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hodac\\AppData\\Local\\Temp\\ipykernel_11164\\4212648226.py:39: DtypeWarning: Columns (1,8,12,17,19,21,22,23,24,25,27,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  datasets.append(pipeline_by_minute(pd.read_csv(dataset_name)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other error:  ../data/sources/app-dependent//2022-07-22/2022-07-22-garbo10-combined.log\n",
      "[Errno 2] No such file or directory: '../data/sources/app-dependent//2022-07-22/2022-07-22-garbo10-combined.log'\n",
      "Finished minutely data\n"
     ]
    }
   ],
   "source": [
    "def generate_daily_dataset():\n",
    "    datasets = []\n",
    "    for date in tqdm(dates):\n",
    "        for truck in trucks:\n",
    "            dataset_name = get_dataset_name(date, truck, base)\n",
    "            try:\n",
    "                datasets.append(pipeline_by_date(pd.read_csv(dataset_name)))\n",
    "            except ValueError as e:\n",
    "                print('ValueError on ', dataset_name)\n",
    "                print(e)\n",
    "            except Exception as e:\n",
    "                print('Other error: ', dataset_name)\n",
    "                print(e)\n",
    "    \n",
    "    return pd.concat(datasets).sort_values(by=['truck', 'Month', 'Date'])\n",
    "\n",
    "def generate_hourly_dataset():\n",
    "    datasets = []\n",
    "    for date in tqdm(dates):\n",
    "        for truck in trucks:\n",
    "            dataset_name = get_dataset_name(date, truck, base)\n",
    "            try:\n",
    "                datasets.append(pipeline_by_hour(pd.read_csv(dataset_name)))\n",
    "            except ValueError as e:\n",
    "                print('ValueError on ', dataset_name)\n",
    "                print(e)\n",
    "            except Exception as e:\n",
    "                print('Other error: ', dataset_name)\n",
    "                print(e)\n",
    "    \n",
    "    return pd.concat(datasets).sort_values(by=['truck', 'Year', 'Month', 'Date', 'hour'])\n",
    "\n",
    "def generate_minutely_dataset():\n",
    "    datasets = []\n",
    "    for date in tqdm(dates):\n",
    "        for truck in trucks:\n",
    "            dataset_name = get_dataset_name(date, truck, base)\n",
    "            try:\n",
    "                datasets.append(pipeline_by_minute(pd.read_csv(dataset_name)))\n",
    "            except ValueError as e:\n",
    "                print('ValueError on ', dataset_name)\n",
    "                print(e)\n",
    "            except Exception as e:\n",
    "                print('Other error: ', dataset_name)\n",
    "                print(e)\n",
    "    \n",
    "    return pd.concat(datasets).sort_values(by=['truck', 'Year', 'Month', 'Date', 'hour', 'min'])\n",
    "\n",
    "def generate_secondly_dataset():\n",
    "    datasets = []\n",
    "    for date in tqdm(dates):\n",
    "        for truck in trucks:\n",
    "            dataset_name = get_dataset_name(date, truck, base)\n",
    "            try:\n",
    "                datasets.append(pipeline_by_second(pd.read_csv(dataset_name)))\n",
    "            except ValueError as e:\n",
    "                print('ValueError on ', dataset_name)\n",
    "                print(e)\n",
    "            except Exception as e:\n",
    "                print('Other error: ', dataset_name)\n",
    "                print(e)\n",
    "    \n",
    "    return pd.concat(datasets).sort_values(by=['truck', 'Year', 'Month', 'Date', 'hour', 'min', 'sec'])\n",
    "    \n",
    "# daily_data = generate_daily_dataset()\n",
    "# daily_data = daily_data.rename(columns=rename_map)\n",
    "# daily_data.to_csv(output_folder + \"date.csv\")\n",
    "# print(\"Finished daily data\")\n",
    "\n",
    "# hourly_data = generate_hourly_dataset()\n",
    "# hourly_data = hourly_data.rename(columns=rename_map)\n",
    "# hourly_data.to_csv(output_folder + \"hour.csv\")\n",
    "# print(\"Finished hourly data\")\n",
    "\n",
    "minutely_data = generate_minutely_dataset()\n",
    "minutely_data = minutely_data.rename(columns=rename_map)\n",
    "minutely_data.to_csv(output_folder + \"minute.csv\")\n",
    "print(\"Finished minutely data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074268e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
